{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Download NLTK stop words\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Load your data\n",
    "# data_transposed = pd.read_excel('indonesia_tourism_reviews.xlsx').transpose()\n",
    "# review_columns = data_transposed.columns[:-1]\n",
    "\n",
    "# # Advanced text preprocessing\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     words = word_tokenize(text)\n",
    "#     words = [w.lower() for w in words if w.isalpha()]  # Remove punctuation and numbers\n",
    "#     words = [w for w in words if not w in stop_words]  # Remove stop words\n",
    "#     return ' '.join(words)\n",
    "\n",
    "# data_transposed['combined_reviews'] = data_transposed[review_columns].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "# data_transposed['combined_reviews'] = data_transposed['combined_reviews'].apply(preprocess_text)\n",
    "\n",
    "# # Concatenate place names with reviews\n",
    "# data_transposed['text_with_place'] = data_transposed.index + ' ' + data_transposed['combined_reviews']\n",
    "\n",
    "# # Prepare the dataset for TensorFlow\n",
    "# texts = data_transposed['text_with_place'].values\n",
    "# labels = data_transposed.index\n",
    "\n",
    "# # Tokenize and pad sequences\n",
    "# tokenizer = Tokenizer(num_words=10000)\n",
    "# tokenizer.fit_on_texts(texts)\n",
    "# sequences = tokenizer.texts_to_sequences(texts)\n",
    "# word_index = tokenizer.word_index\n",
    "# data = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# # Load GloVe embeddings\n",
    "# def load_glove_embeddings(glove_path, word_index, embedding_dim=100):\n",
    "#     embeddings_index = {}\n",
    "#     with open(glove_path, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "\n",
    "#     embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "#     for word, i in word_index.items():\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#     return embedding_matrix\n",
    "\n",
    "# glove_path = 'glove.6B.100d.txt'  # Ensure you have the GloVe embeddings file\n",
    "# embedding_dim = 100\n",
    "# embedding_matrix = load_glove_embeddings(glove_path, word_index, embedding_dim)\n",
    "\n",
    "# # Improved Model Architecture\n",
    "# inputs = tf.keras.Input(shape=(200,))\n",
    "# x = tf.keras.layers.Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "# x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "# outputs = tf.keras.layers.Dense(embedding_dim)(x)\n",
    "\n",
    "# model = tf.keras.Model(inputs, outputs)\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the Model\n",
    "# text_embeddings = model.predict(data)\n",
    "\n",
    "# similarity_scores_dict = {}\n",
    "\n",
    "# review_df= pd.read_excel('indonesia_tourism_reviews.xlsx')\n",
    "\n",
    "# # Recommendations\n",
    "# def get_similarity_scores(selected_text_embedding, text_embeddings):\n",
    "#     similarity_scores = cosine_similarity(selected_text_embedding.reshape(1, -1), text_embeddings)\n",
    "#     return similarity_scores\n",
    "\n",
    "# selected_place = 'Museum Pendidikan Nasional UPI'\n",
    "# selected_place_index = labels.get_loc(selected_place)\n",
    "# selected_place_embedding = text_embeddings[selected_place_index]\n",
    "\n",
    "# similarity_scores = get_similarity_scores(selected_place_embedding, text_embeddings)\n",
    "\n",
    "# sorted_similarities = sorted(list(enumerate(similarity_scores[0])), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Display top 5 recommendations with their similarity percentages (excluding the selected place itself)\n",
    "# top_recommendations_with_scores = [(labels[i[0]], i[1] * 100) for i in sorted_similarities[1:10]]\n",
    "# print(top_recommendations_with_scores)\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# # Enable Select TensorFlow ops\n",
    "# converter.target_spec.supported_ops = [\n",
    "#     tf.lite.OpsSet.TFLITE_BUILTINS,  # TFLite built-in ops.\n",
    "#     tf.lite.OpsSet.SELECT_TF_OPS     # Select TensorFlow ops.\n",
    "# ]\n",
    "\n",
    "# # Disable experimental lowering of TensorList ops\n",
    "# converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# # Convert the model\n",
    "# try:\n",
    "#     tflite_model = converter.convert()\n",
    "#     # Save the TFLite model\n",
    "#     with open('model.tflite', 'wb') as f:\n",
    "#         f.write(tflite_model)\n",
    "#     print(\"Model conversion successful!\")\n",
    "# except Exception as e:\n",
    "#     print(\"Model conversion failed:\", e)\n",
    "# # tflite_model = converter.convert()\n",
    "\n",
    "# # Save the tokenizer\n",
    "# import json\n",
    "\n",
    "# tokenizer_json = tokenizer.to_json()\n",
    "# with open('tokenizer.json', 'w') as f:\n",
    "#     f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_excel('indonesia_tourism_reviews.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text preprocessing\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words if w.isalpha()]  # Remove punctuation and numbers\n",
    "    words = [w for w in words if not w in stop_words]  # Remove stop words\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine reviews from the specified columns\n",
    "review_columns = ['review1', 'review2', 'review3', 'review4', 'review5']\n",
    "data['combined_reviews'] = data[review_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "data['combined_reviews'] = data['combined_reviews'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate place names with reviews\n",
    "data['text_with_place'] = data['Place_Name'] + ' ' + data['combined_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "data_sequences = pad_sequences(sequences, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_path, word_index, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'glove.6B.100d.txt'  # Ensure you have the GloVe embeddings file\n",
    "embedding_dim = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_path, word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Model Architecture\n",
    "inputs = tf.keras.Input(shape=(200,))\n",
    "x = tf.keras.layers.Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(embedding_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "text_embeddings = model.predict(data_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate similarity scores for all destinations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m(text_embeddings)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cosine_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate similarity scores for all destinations\n",
    "similarity_matrix = cosine_similarity(text_embeddings)\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=labels, columns=labels)\n",
    "\n",
    "# Save the similarity matrix to an Excel file\n",
    "similarity_df.to_excel('tourism_similarity_matrix.xlsx')\n",
    "\n",
    "print(\"Similarity matrix saved to 'tourism_similarity_matrix.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriks berhasil disimpan dalam file destination_matrix.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Baca file CSV\n",
    "df = pd.read_csv('similarities_description_based.csv')\n",
    "\n",
    "# Buat daftar unik dari Place dan Similar Place\n",
    "places = list(set(df['Place']).union(set(df['Similar Place'])))\n",
    "\n",
    "# Inisialisasi matriks dengan nilai 0\n",
    "matrix = pd.DataFrame(0, index=places, columns=places)\n",
    "\n",
    "# Isi matriks dengan nilai Score\n",
    "for _, row in df.iterrows():\n",
    "    matrix.at[row['Place'], row['Similar Place']] = row['Score']\n",
    "\n",
    "# Simpan matriks ke dalam file Excel\n",
    "matrix.to_excel('desc_matrix.xlsx', engine='openpyxl')\n",
    "\n",
    "print(\"Matriks berhasil disimpan dalam file destination_matrix.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Baca file Excel\n",
    "df = pd.read_excel('indonesia_tourism.xlsx')\n",
    "\n",
    "# Inisialisasi pipeline untuk analisis sentimen\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
    "\n",
    "# Fungsi untuk melakukan analisis sentimen pada satu review\n",
    "def analyze_sentiment(review):\n",
    "    if pd.isna(review):  # Jika review adalah NaN, kembalikan None\n",
    "        return None\n",
    "    result = sentiment_pipe(review)\n",
    "    return result[0]['label']\n",
    "\n",
    "# Daftar kolom review\n",
    "review_columns = [f'Review{i}' for i in range(1, 6)]\n",
    "\n",
    "# Lakukan analisis sentimen pada setiap kolom Review1 hingga Review5\n",
    "for review_col in review_columns:\n",
    "    sentiment_col = f'Sentiment_{review_col}'\n",
    "    df[sentiment_col] = df[review_col].apply(analyze_sentiment)\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "df.to_excel('indonesia_tourism_with_sentiments.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "print(\"Analisis sentimen selesai dan hasil disimpan dalam file indonesia_tourism_with_sentiments.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
